## Week 3: RLHF

### Hugging Face: Illustrating Reinforcement Learning from Human Feedback (RLHF)

3 Steps: Pretraining, gathering data and training reward model, and fine-tuning
model with reinforcement learning

RLHF traditionally takes language model pretained with classical objectives

Reward Model Training: 

goal - get model/system that takes sequence of text and returns scalar to represent human preference

can be another fine-tuned LM or LM trained from scratch on preference data

training dataset for RM generated by sampling prompts from dataset like Mechanical Turk

Human annotators rank text outputs 
- option: Elo system where two models are put head-to-head

**Pretraining**

### Constitutional AI

### Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback
